---
# Artemis Custom Runner Configuration
# For dbt SQL Optimization Benchmarking

name: "dbt-sql-optimization-benchmarking"
version: "1.0"
description: "Compile, test, benchmark dbt pipelines for SQL optimization (Pipeline A, B, C)"

# Build settings for dbt compilation and testing
build:
  # Pre-build steps (prepare environment)
  before:
    - name: "Check dbt version"
      command: "dbt --version"
      description: "Verify dbt CLI is installed"

    - name: "Verify profiles.yml"
      command: "test -f profiles.yml && echo '✅ profiles.yml found' || echo '❌ Missing profiles.yml'"
      description: "Check Snowflake connection configuration"

  # Main build steps (compile and test)
  main:
    - name: "dbt-parse"
      command: "dbt parse"
      description: "Parse dbt project and validate YAML"

    - name: "dbt-compile"
      command: "dbt compile --threads 4"
      description: "Compile all dbt models to SQL"
      timeout: 600

    - name: "dbt-deps"
      command: "dbt deps"
      description: "Install dbt package dependencies"

  # Post-build steps (test and benchmark)
  after:
    - name: "dbt-test-all"
      command: "dbt test --threads 4"
      description: "Run all data quality tests"
      timeout: 900
      on_failure: "warn"

# Benchmarking configuration
benchmark:
  enabled: true

  # Baseline generation (current state)
  baseline:
    - name: "baseline-pipeline-a"
      steps:
        - command: "dbt seed"
          description: "Load seed data into Snowflake"

        - command: "dbt run --select pipeline_a --threads 4"
          description: "Run Pipeline A (Simple)"
          timeout: 600

        - command: "python extract_report.py --pipeline a"
          description: "Extract baseline KPI metrics for Pipeline A"

      artifacts:
        - "benchmark/pipeline_a/baseline/report.json"

      metadata:
        pipeline: "A"
        complexity: "simple"
        models: 4
        expected_runtime: "3.2s"

    - name: "baseline-pipeline-b"
      steps:
        - command: "dbt run --select pipeline_b --threads 4"
          description: "Run Pipeline B (Medium)"
          timeout: 600

        - command: "python extract_report.py --pipeline b"
          description: "Extract baseline KPI metrics for Pipeline B"

      artifacts:
        - "benchmark/pipeline_b/baseline/report.json"

      metadata:
        pipeline: "B"
        complexity: "medium"
        models: 12
        expected_runtime: "12.7s"

    - name: "baseline-pipeline-c"
      steps:
        - command: "dbt run --select pipeline_c --threads 4"
          description: "Run Pipeline C (Complex)"
          timeout: 900

        - command: "python extract_report.py --pipeline c"
          description: "Extract baseline KPI metrics for Pipeline C"

      artifacts:
        - "benchmark/pipeline_c/baseline/report.json"

      metadata:
        pipeline: "C"
        complexity: "complex"
        models: 20
        expected_runtime: "48.3s"

  # Candidate generation (after optimization)
  candidate:
    enabled: false  # Enable after optimizing SQL

    - name: "candidate-pipeline-a"
      steps:
        - command: "dbt run --select pipeline_a --threads 4"
          description: "Re-run optimized Pipeline A"
          timeout: 600

        - command: "python extract_report.py --pipeline a --output benchmark/pipeline_a/candidate/report.json"
          description: "Extract candidate KPI metrics for Pipeline A"

      artifacts:
        - "benchmark/pipeline_a/candidate/report.json"

    - name: "candidate-pipeline-b"
      steps:
        - command: "dbt run --select pipeline_b --threads 4"
          description: "Re-run optimized Pipeline B"
          timeout: 600

        - command: "python extract_report.py --pipeline b --output benchmark/pipeline_b/candidate/report.json"
          description: "Extract candidate KPI metrics for Pipeline B"

      artifacts:
        - "benchmark/pipeline_b/candidate/report.json"

    - name: "candidate-pipeline-c"
      steps:
        - command: "dbt run --select pipeline_c --threads 4"
          description: "Re-run optimized Pipeline C"
          timeout: 900

        - command: "python extract_report.py --pipeline c --output benchmark/pipeline_c/candidate/report.json"
          description: "Extract candidate KPI metrics for Pipeline C"

      artifacts:
        - "benchmark/pipeline_c/candidate/report.json"

  # Comparison and analysis
  comparison:
    - name: "compare-pipeline-a"
      command: "python benchmark/compare_kpis.py benchmark/pipeline_a/baseline/report.json benchmark/pipeline_a/candidate/report.json"
      description: "Compare baseline vs candidate for Pipeline A"
      enabled_when: "candidate.enabled == true"
      timeout: 60

    - name: "compare-pipeline-b"
      command: "python benchmark/compare_kpis.py benchmark/pipeline_b/baseline/report.json benchmark/pipeline_b/candidate/report.json"
      description: "Compare baseline vs candidate for Pipeline B"
      enabled_when: "candidate.enabled == true"
      timeout: 60

    - name: "compare-pipeline-c"
      command: "python benchmark/compare_kpis.py benchmark/pipeline_c/baseline/report.json benchmark/pipeline_c/candidate/report.json"
      description: "Compare baseline vs candidate for Pipeline C"
      enabled_when: "candidate.enabled == true"
      timeout: 60

# Settings for build output and artifacts
settings:
  # Output configuration
  output:
    format: "json"  # or "text", "html"
    verbosity: "detailed"  # or "normal", "quiet"

    # Save build artifacts
    artifacts:
      enabled: true
      path: "artemis_output/"
      include:
        - "dbt_compile_output/"
        - "benchmark/pipeline_a/baseline/"
        - "benchmark/pipeline_b/baseline/"
        - "benchmark/pipeline_c/baseline/"
        - "benchmark/pipeline_a/candidate/"
        - "benchmark/pipeline_b/candidate/"
        - "benchmark/pipeline_c/candidate/"

  # Notifications
  notifications:
    on_success: true
    on_failure: true
    channels:
      - type: "console"

  # Resource limits
  resources:
    max_runtime_seconds: 3600
    max_memory_mb: 4096
    thread_pool_size: 4

# Optimization flags for Artemis analysis
optimization:
  enabled: true

  # Targets for analysis
  targets:
    - name: "pipeline_a_fact_cashflow"
      model: "fact_cashflow_summary"
      optimization_focus: "aggregation_pushdown"

    - name: "pipeline_b_fact_trades"
      model: "fact_trade_execution"
      optimization_focus: "join_consolidation"

    - name: "pipeline_c_fact_portfolio"
      model: "fact_portfolio_performance"
      optimization_focus: "window_function_consolidation"

  # Metrics to collect
  metrics:
    - "execution_time"
    - "bytes_scanned"
    - "complexity_score"
    - "estimated_credits"
    - "query_plan"

# Validation rules
validation:
  # Ensure output equivalence
  output_hash:
    enabled: true
    algorithm: "sha256"
    compare: "baseline_vs_candidate"

  # Data quality checks
  data_quality:
    enabled: true
    checks:
      - type: "not_null"
        columns: ["cashflow_summary_key"]
      - type: "unique"
        columns: ["cashflow_summary_key"]
      - type: "accepted_values"
        column: "cashflow_type"
        values: ["CONTRIBUTION", "DISTRIBUTION", "FEE", "DIVIDEND"]

# Reporting configuration
reporting:
  enabled: true

  # Report types
  reports:
    - type: "benchmark_summary"
      output: "artemis_output/benchmark_summary.json"

    - type: "kpi_comparison"
      output: "artemis_output/kpi_comparison.html"

    - type: "optimization_recommendations"
      output: "artemis_output/recommendations.md"

---
# Additional Configuration Notes

# To use this configuration with Artemis Custom Runner:
# 1. Place this file in the project root
# 2. Configure Snowflake connection in profiles.yml
# 3. Run with Artemis: artemis build --config artemis.yaml

# Quick command reference:
#
# Compile and test only:
#   artemis build --config artemis.yaml --skip-benchmark
#
# Generate baseline:
#   artemis build --config artemis.yaml --target baseline
#
# Generate candidate (after optimization):
#   artemis build --config artemis.yaml --target candidate
#
# Full comparison:
#   artemis build --config artemis.yaml --include-comparison
#
# Specific pipeline:
#   artemis build --config artemis.yaml --pipeline a
#   artemis build --config artemis.yaml --pipeline b
#   artemis build --config artemis.yaml --pipeline c
